{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201e0b01",
   "metadata": {},
   "source": [
    "# Preprocess for training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb197dd5",
   "metadata": {},
   "source": [
    "Sentinel-2 images need to be processed to correct invalid data, mask the clouds, derive NDVI index, superresolve, coregister to reduce small offsets between acquisitions, export the numpy arrays as compressed arrays. Geotiff images have also been produced just for a check.\n",
    "Functions defined for the main tasks are applied both for the training and test set. The only differences between them are the number of images and the way to access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0294c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eurodatacube-gpu-0.24.5/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Jupyter notebook related\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7132feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eurodatacube-gpu-0.24.5/lib/python3.8/site-packages/bolt/utils.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n",
      "/opt/conda/envs/eurodatacube-gpu-0.24.5/lib/python3.8/site-packages/bolt/factory.py:17: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args = inspect.getargspec(func)\n"
     ]
    }
   ],
   "source": [
    "# Built-in modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime as dt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Module for GeoDB\n",
    "from xcube_geodb.core.geodb import GeoDBClient\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from sentinelhub import CRS, BBox, SHConfig, DataCollection\n",
    "from eolearn.core import (FeatureType,\n",
    "                          EOPatch, \n",
    "                          EOTask, \n",
    "                          LinearWorkflow, \n",
    "                          EOExecutor, \n",
    "                          LoadTask,\n",
    "                          SaveTask)\n",
    "from eolearn.io import GeoDBVectorImportTask, SentinelHubInputTask\n",
    "from eolearn.geometry import VectorToRaster\n",
    "from eolearn.coregistration import ThunderRegistration\n",
    "from eolearn.io import ExportToTiff\n",
    "from eolearn.core import AddFeature\n",
    "\n",
    "# Other libraries for image processing\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.transform import resize, warp\n",
    "import scipy.ndimage\n",
    "from registration import CrossCorr\n",
    "register = CrossCorr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67323d96",
   "metadata": {},
   "source": [
    "### Connection parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e637f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_config = SHConfig()\n",
    "\n",
    "sh_config.sh_client_id = '...'\n",
    "sh_config.sh_client_secret = '...'\n",
    "\n",
    "geodb_client_id = '...'\n",
    "geodb_client_secret = '...'\n",
    "\n",
    "client = GeoDBClient()\n",
    "\n",
    "EOPATCHES_PATH = 'eopatches'\n",
    "EOPATCHES_TRAIN_PATH = f'{EOPATCHES_PATH}/train/'\n",
    "EOPATCHES_TEST_PATH = f'{EOPATCHES_PATH}/test/'\n",
    "\n",
    "SUBMISSION_DIR = 'submission'\n",
    "if not os.path.exists(SUBMISSION_DIR):\n",
    "    os.makedirs(SUBMISSION_DIR)\n",
    "\n",
    "# DO NOT CHANGE THIS\n",
    "GEODB_DATABASE = 'geodb_0e5d743f-2134-4561-8946-a073b039176f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e51171",
   "metadata": {},
   "source": [
    "### Area of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a050b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = client.get_collection('ai4eo_bboxes', database=GEODB_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f565042",
   "metadata": {},
   "source": [
    "### Sentinel-2 time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download parameters - DO NOT CHANGE\n",
    "S2_TIME_INTERVAL = ('2019-03-01','2019-09-01')\n",
    "\n",
    "S2_RESOLUTION = 10  # metres\n",
    "S2_MAXCC = 0.5\n",
    "S2_TIME_DELTA = 120\n",
    "THRES = 200\n",
    "\n",
    "MAX_THREADS = 5\n",
    "\n",
    "# process parameters\n",
    "MIN_CLOUD_PROB = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c5be6",
   "metadata": {},
   "source": [
    "### Definition of the main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab889e",
   "metadata": {},
   "source": [
    "##### Function for the interpolation of pixels along the time period in case of cloudy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda3e3dc-21cb-442b-83d8-dcb34bc7ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(ndvi_in, valid_in, n_frames):\n",
    "    x = np.linspace(0,n_frames-1,n_frames).astype(int)    \n",
    "    ndvi_tmp = np.zeros((n_frames, 500, 500), dtype=np.int)\n",
    "\n",
    "    #for every pixel along the time series\n",
    "    for r in range(500):\n",
    "        for c in range(500):\n",
    "            \n",
    "            # list of values for each pixel stored as ndvi2\n",
    "            ndvi2 = ndvi_in[:,r,c].astype(int)   \n",
    "\n",
    "            # if there is at least one invalid pixel then interpolate\n",
    "            if sum(valid_in[:,r,c]) > 0:\n",
    "                \n",
    "                invalid = valid_in[:,r,c].nonzero()[0]  \n",
    "                ndvi_ma = np.ma.masked_array(ndvi_in[:,r,c],valid_in[:,r,c]).compressed()\n",
    "                x_ma = np.ma.masked_array(x, valid_in[:,r,c]).compressed()\n",
    "                ndvi_new = np.interp(invalid, x_ma, ndvi_ma)     \n",
    "                               \n",
    "                for a in invalid:\n",
    "                    pos = np.where(invalid == a)\n",
    "                    ndvi2[a] = ndvi_new[pos].astype(int)\n",
    "\n",
    "            ndvi_tmp[:,r,c] = ndvi2\n",
    "     \n",
    "    return ndvi_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe6216",
   "metadata": {},
   "source": [
    "##### Function for the detection of the best reference cloud-free image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07f80dc-ecef-4261-8b68-e6366e01be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose of the best reference image as completely cloud-free starting from the last\n",
    "def find_ref(clm_tmp, n_frames):\n",
    "    for t in range(n_frames-1, -1, -1): \n",
    "        if np.sum(clm_tmp[t,...]) == 0:\n",
    "            break\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d28df",
   "metadata": {},
   "source": [
    "##### Function for the coregistration and the images superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d21a8f5-5f27-49a0-a57b-8cb0f3cce2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coregistration task use the Cross-Correlation function of scikit-image\n",
    "def coreg(ndvi_ds, tref, n_frames):\n",
    "    #superres and coregister images\n",
    "    reference = ndvi_ds[tref,...]\n",
    "    \n",
    "    ref = scipy.ndimage.zoom(reference, 4, order=3)\n",
    "    imgs = [scipy.ndimage.zoom(ndvi_ds[s1,...], 4, order=3) for s1 in range(n_frames)]\n",
    "    model = register.fit(imgs, reference=ref)\n",
    "\n",
    "    shifted_tmp = model.transform(imgs)\n",
    "    shifted_img = shifted_tmp.toarray()\n",
    "\n",
    "    # consider the original image in case of wrong shifts\n",
    "    for s2 in range(n_frames):\n",
    "        dx = abs(int(str(model.transformations[(s2,)]).replace(\"Displacement(delta=[\",\"\").replace(\"])\",\"\").split(\",\")[0]))\n",
    "        dy = abs(int(str(model.transformations[(s2,)]).replace(\"Displacement(delta=[\",\"\").replace(\"])\",\"\").split(\",\")[1]))\n",
    "        if (dx > 10) | (dy > 10):\n",
    "            #print(s2, dx, dy)\n",
    "            shifted_img[s2,...] = scipy.ndimage.zoom(ndvi_ds[s2,...], 4, order=3)\n",
    "    return shifted_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19757c-bf3b-40c3-b514-323f05d3d58d",
   "metadata": {},
   "source": [
    "### Preprocessing of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cf610-871a-4dc3-9bea-55c3f44adbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_hi_sd = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "ndvi_hi_max = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "ndvi_hi_msk = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "\n",
    "for i in tqdm(range(100)):  \n",
    "    \n",
    "    i1 = \"0\" + str(i) if i < 10 else str(i) \n",
    "\n",
    "    bbox = BBox(bboxes.iloc[i].geometry, crs=CRS(bboxes.crs))\n",
    "    id_values = bboxes.eop_index.values[i]\n",
    "\n",
    "    # get the training set\n",
    "    get_s2_l2a = SentinelHubInputTask(\n",
    "        bands_feature=(FeatureType.DATA, 'BANDS'),\n",
    "        bands_dtype=np.uint16,\n",
    "        resolution=S2_RESOLUTION,\n",
    "        maxcc=S2_MAXCC,\n",
    "        time_difference=dt.timedelta(minutes=S2_TIME_DELTA),\n",
    "        data_collection=DataCollection.SENTINEL2_L2A,\n",
    "        additional_data=[(FeatureType.MASK, 'dataMask', 'IS_DATA'),\n",
    "                         (FeatureType.MASK, 'SCL'),\n",
    "                         (FeatureType.MASK, 'CLM'),\n",
    "                         (FeatureType.DATA, 'CLP')],\n",
    "        max_threads=MAX_THREADS,\n",
    "        config=sh_config\n",
    "    )\n",
    "    \n",
    "    s2_l2a_eop = get_s2_l2a.execute(bbox=bbox, time_interval = S2_TIME_INTERVAL)\n",
    "    \n",
    "    #get the number of acquisitions\n",
    "    n_imgs = s2_l2a_eop.data['BANDS'].shape[0]\n",
    "    print(\"box\", i, \"downloaded -\",s2_l2a_eop.data['BANDS'][...,3].shape, end = ' .. ')\n",
    "    \n",
    "    #definition of some band used in the processing\n",
    "    vis_factor = 3.5\n",
    "    norm_factor = s2_l2a_eop.scalar['NORM_FACTORS']\n",
    "    b4 = s2_l2a_eop.data['BANDS'][...,3] * norm_factor[:,None]\n",
    "    b8 = s2_l2a_eop.data['BANDS'][...,7] * norm_factor[:,None]\n",
    "    isdata = s2_l2a_eop.mask['IS_DATA'][...,0]\n",
    "    clm = s2_l2a_eop.mask['CLM'][...,0]\n",
    "    clp = s2_l2a_eop.data['CLP'][...,0]\n",
    "    scl = s2_l2a_eop.mask['SCL'][...,0]\n",
    "    \n",
    "    #valid = ~isdata | clm | (clp > MIN_CLOUD_PROB)\n",
    "    #valid = ~isdata | clm | (scl < 4)|(scl > 6)| (clp > MIN_CLOUD_PROB)\n",
    "    #valid = ~isdata | clm | (clp > MIN_CLOUD_PROB)    \n",
    "    valid = ~isdata | clm \n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    ndvi = np.around(255*b8/(b8+b4))\n",
    "    ndvi = np.nan_to_num(ndvi, copy=False).astype(np.int)\n",
    "    \n",
    "    ndvi_lo = np.zeros((n_imgs, 500, 500), dtype=np.int)\n",
    "    ndvi_hi = np.zeros((n_imgs, 2000, 2000), dtype=np.int)\n",
    "    \n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # call interpolation function and store results in ndvi_lo \n",
    "    ndvi_lo = interpolate(ndvi, valid, n_imgs)\n",
    "    print(\"interpolation\", end = ' .. ')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # find best reference image\n",
    "    ref_img = find_ref(clm, n_imgs)\n",
    "    print(\"found ref img\", ref_img, end = ' .. ')\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # superresolve and coregister\n",
    "    ndvi_hi = coreg(ndvi_lo, ref_img, n_imgs)\n",
    "    print(\"superres\", ndvi_hi.shape, end = ' .. ')\n",
    "     \n",
    "    # --------------------------------------------------------\n",
    "    # stdev of ndvi along all images\n",
    "    ndvi_hi_sd[...,0] = np.std(ndvi_hi, axis=0)\n",
    "    ndvi_hi_max[...,0] = np.amax(ndvi_hi, axis=0)\n",
    "    ndvi_hi_msk[...,0]= np.where(ndvi_hi_max[...,0] < THRES, 0, 1)\n",
    "   \n",
    "    print(\"stdev\", end = ' .. ')\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    #save to numpy compressed and geotiff  \n",
    "    new_eopatch = EOPatch(bbox=bbox)\n",
    "    ndvisd = (FeatureType.DATA_TIMELESS, 'ndvi_sd')\n",
    "    add_feature = AddFeature(ndvisd)\n",
    "    new_eopatch = add_feature.execute(new_eopatch, ndvi_hi_sd)\n",
    "    \n",
    "    task = ExportToTiff((FeatureType.DATA_TIMELESS, 'ndvi_sd'), folder='geotiff/train', band_indices=[0]) #, band_indices=[1]\n",
    "    task.execute(new_eopatch, filename = \"eopatch-\" + i1 + \".tiff\")\n",
    "  \n",
    "    np.savez_compressed(\"./data/train/\" + i1, ndvi_hi_sd[...,0])\n",
    "\n",
    "\n",
    "    print(\"saved compressed\", end = '\\n')\n",
    "\n",
    "    \n",
    "print(\"End of preprocessing for the training set\", end = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d84c7-782c-456b-a38f-49dc02b85897",
   "metadata": {},
   "source": [
    "### Preprocessing of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c5932-d526-4d03-adb8-bda13ecd08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_hi_sd = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "ndvi_hi_max = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "ndvi_hi_msk = np.zeros((2000, 2000, 1), dtype=np.int)\n",
    "\n",
    "for i in tqdm(range(1,26)):  \n",
    "    \n",
    "    i1 = \"0\" + str(i) if i < 10 else str(i) \n",
    "    i2 = \"0\" + str(i-1) if i-1 < 10 else str(i-1)\n",
    "    load = LoadTask(path=EOPATCHES_TEST_PATH)\n",
    "\n",
    "    #load the test set from the local folder\n",
    "    eops_test = sorted(os.listdir(f'{EOPATCHES_PATH}/test/'))\n",
    "    s2_l2a_eop = load.execute(eopatch_folder = f'eopatch-' + i1)  \n",
    "    bbox = s2_l2a_eop.bbox\n",
    "    \n",
    "    #get the number of acquisitions\n",
    "    n_imgs = s2_l2a_eop.data['BANDS'].shape[0]\n",
    "    print(\"box\", i, \"downloaded -\",s2_l2a_eop.data['BANDS'][...,3].shape, end = ' .. ')\n",
    "    \n",
    "    #definition of some band used in the processing\n",
    "    vis_factor = 3.5\n",
    "    norm_factor = s2_l2a_eop.scalar['NORM_FACTORS']\n",
    "    b4 = s2_l2a_eop.data['BANDS'][...,3] * norm_factor[:,None]\n",
    "    b8 = s2_l2a_eop.data['BANDS'][...,7] * norm_factor[:,None]\n",
    "    isdata = s2_l2a_eop.mask['IS_DATA'][...,0]\n",
    "    clm = s2_l2a_eop.mask['CLM'][...,0]\n",
    "    clp = s2_l2a_eop.data['CLP'][...,0]\n",
    "    scl = s2_l2a_eop.mask['SCL'][...,0]\n",
    "    \n",
    "    #valid = ~isdata | clm | (clp > MIN_CLOUD_PROB)\n",
    "    #valid = ~isdata | clm | (scl < 4)|(scl > 6)| (clp > MIN_CLOUD_PROB)\n",
    "    #valid = ~isdata | clm | (clp > MIN_CLOUD_PROB)    \n",
    "    valid = ~isdata | clm \n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    ndvi = np.around(255*b8/(b8+b4))\n",
    "    ndvi = np.nan_to_num(ndvi, copy=False).astype(np.int)\n",
    "    \n",
    "    ndvi_lo = np.zeros((n_imgs, 500, 500), dtype=np.int)\n",
    "    ndvi_hi = np.zeros((n_imgs, 2000, 2000), dtype=np.int)\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # call interpolation function and store results in ndvi_lo \n",
    "    \n",
    "    ndvi_lo = interpolate(ndvi, valid, n_imgs)\n",
    "    print(\"interpolation\", end = ' .. ')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # find best reference image\n",
    "    \n",
    "    ref_img = find_ref(clm, n_imgs)\n",
    "    print(\"found ref img\", ref_img, end = ' .. ')\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # superresolve and coregister\n",
    "    \n",
    "    ndvi_hi = coreg(ndvi_lo, ref_img, n_imgs)    \n",
    "    print(\"superres\", ndvi_hi.shape, end = ' .. ')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # stdev of ndvi along all images\n",
    "    \n",
    "    ndvi_hi_sd[...,0] = np.std(ndvi_hi, axis=0)\n",
    "    ndvi_hi_max[...,0] = np.amax(ndvi_hi, axis=0)\n",
    "    ndvi_hi_msk[...,0]= np.where(ndvi_hi_max[...,0] < THRES, 0, 1)\n",
    "    print(\"stdev and mean\", end = ' .. ')\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    #save to numpy compressed and geotiff  \n",
    "    \n",
    "    new_eopatch = EOPatch(bbox=bbox)\n",
    "    ndvisd = (FeatureType.DATA_TIMELESS, 'ndvi_sd')\n",
    "    add_feature = AddFeature(ndvisd)\n",
    "    new_eopatch = add_feature.execute(new_eopatch, ndvi_hi_sd)\n",
    "    \n",
    "    task = ExportToTiff((FeatureType.DATA_TIMELESS, 'ndvi_sd'), folder='geotiff/test', band_indices=[0]) #, band_indices=[1]\n",
    "    task.execute(new_eopatch, filename = \"eopatch-\" + i1 + \".tif\")\n",
    "  \n",
    "    np.savez_compressed(\"./data/test/\" + i2, ndvi_hi_sd[...,0])\n",
    "\n",
    "\n",
    "    print(\"saved compressed\", end = '\\n')\n",
    "\n",
    "    \n",
    "print(\"End of preprocessing for the test set\", end = '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
